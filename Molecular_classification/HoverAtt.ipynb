{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torchvision import models as torch_models1\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from PIL import Image\n",
    "from scipy import interp\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import pandas as pd\n",
    "from torch.utils.data import Sampler, BatchSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, confusion_matrix, precision_recall_curve\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.metrics import auc as calc_auc\n",
    "from topk.svm import SmoothTop1SVM\n",
    "from torch_kmeans import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = pd.read_excel('/home/ldap_howard/script/MSI_CRC_DX_0307.xlsx',sheet_name = 'MSIHMSS')\n",
    "train_dataset = pd.read_excel('/home/ldap_howard/script/Gene_CRC_DX_0307.xlsx',sheet_name='TP53')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_dataset['Patients'].values\n",
    "train_y = train_dataset['isMSIH']\n",
    "train_y = train_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npy_loader(path):\n",
    "    x = np.load(path,allow_pickle=True).item()\n",
    "    x_im = torch.from_numpy(x['features'])\n",
    "    return x_im\n",
    "\n",
    "def npy_loader_count(path):\n",
    "    x = np.load(path,allow_pickle=True).item()\n",
    "    x_im = torch.from_numpy(x['counts'])\n",
    "    #x_im = x_im[:,1:3]\n",
    "    return x_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.path = '/ORCA_lake/TCGA-COAD/feature/CRC_resnet0307/'\n",
    "        self.path_c = '/ORCA_lake/TCGA-COAD/hovernet_kmeans/CRC_0307_MI2N/'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)   \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.path, self.x[idx]+'.npy')\n",
    "        count_path = os.path.join(self.path_c, self.x[idx]+'.npy')\n",
    "\n",
    "        return image_path, count_path, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight=None, \n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, logits, label):\n",
    "        log_prob = F.log_softmax(logits, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob, \n",
    "            label, \n",
    "            weight=self.weight,\n",
    "            reduction = self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, L=2048, D=1024, dropout=True, n_classes=2, top_k=1, instance_loss_fn=FocalLoss()):\n",
    "        super(Attention, self).__init__()\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.K = 1\n",
    "\n",
    "        self.layer1 = nn.Linear(self.L, self.D)\n",
    "        if dropout:\n",
    "            self.attention_V = nn.Sequential(nn.Linear(self.D, 512), nn.Tanh(), nn.Dropout(0.25))\n",
    "            self.attention_U = nn.Sequential(nn.Linear(self.D, 512), nn.Sigmoid(), nn.Dropout(0.25))\n",
    "        else:\n",
    "            self.attention_V = nn.Sequential(nn.Linear(self.D, 512), nn.Tanh())\n",
    "            self.attention_U = nn.Sequential(nn.Linear(self.D, 512), nn.Sigmoid())\n",
    "\n",
    "        self.attention_weights = nn.Linear(512, self.K)\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Linear(self.D,1024),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.25),\n",
    "                                        nn.Linear(1024, 512),\n",
    "                                        nn.ReLU(), \n",
    "                                        nn.Linear(512,2),\n",
    "                                        nn.Sigmoid())\n",
    "        self.top_k = top_k\n",
    "        self.instance_loss = instance_loss_fn\n",
    "        self.fc_X = nn.Sequential(nn.Linear(1, 2), nn.Sigmoid())\n",
    "        self.fc_c = nn.Sequential(nn.Linear(4, 2), nn.Sigmoid())\n",
    "\n",
    "    def relocate(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.classifier.to(device)\n",
    "        self.attention_V.to(device)\n",
    "        self.attention_U.to(device)\n",
    "        self.attention_weights.to(device)\n",
    "        self.instance_loss.to(device)\n",
    "        self.fc_c.to(device)\n",
    "        self.fc_X.to(device)\n",
    "    \n",
    "    def inst_eval(self, A_T, count):\n",
    "        logits_c = self.fc_c(count)\n",
    "        hover_logits = torch.mm(A_T, logits_c)\n",
    "        y_probs_c = F.softmax(logits_c, dim=1)\n",
    "        k = math.ceil(logits_c.size()[0] / 20)\n",
    "        _, predicted_class = torch.max(y_probs_c, dim=1)\n",
    "        predicted_prob = y_probs_c[torch.arange(y_probs_c.size(0)), predicted_class]\n",
    "        top_instance_idx = torch.topk(predicted_prob, 5, largest=True)[1]\n",
    "        top_instance = torch.index_select(y_probs_c, dim=0, index=top_instance_idx)\n",
    "        _, pseudo_targets = torch.max(top_instance, dim=1)\n",
    "\n",
    "        A_T = torch.transpose(A_T, 1, 0)  # KxN\n",
    "        logits_x = self.fc_X(A_T)\n",
    "        y_probs_x = F.softmax(logits_x, dim=1)\n",
    "        top_instance_x = torch.index_select(logits_x, dim=0, index=top_instance_idx)\n",
    "        pseudo_logits = top_instance_x\n",
    "\n",
    "        return pseudo_logits, pseudo_targets,  hover_logits\n",
    "\n",
    "    def forward(self, x, count, eval=False):\n",
    "        x = self.layer1(x)\n",
    "        A_V = self.attention_V(x)  # NxD\n",
    "        A_U = self.attention_U(x)\n",
    "        A = self.attention_weights(A_V*A_U)\n",
    "        A_T = torch.transpose(A, 1, 0)  # KxN\n",
    "        A_T = F.softmax(A_T, dim=1)  # softmax over N\n",
    "        M = torch.mm(A_T, x)  # KxL\n",
    "\n",
    "        if eval == False:\n",
    "            pseudo_logits, pseudo_targets, hover_logits = self.inst_eval(A_T, count)\n",
    "            instance_loss = self.instance_loss(pseudo_logits, pseudo_targets)\n",
    "       \n",
    "        logits = self.classifier(M)\n",
    "        y_probs = F.softmax(logits, dim=1)\n",
    "        top_instance_idx = torch.topk(y_probs[:, 1], self.top_k, dim=0)[1].view(1, )\n",
    "        top_instance = torch.index_select(logits, dim=0, index=top_instance_idx)\n",
    "        Y_hat = torch.topk(top_instance, 1, dim=1)[1]\n",
    "        Y_prob = F.softmax(top_instance, dim=1)\n",
    "        results_dict = {}\n",
    "        if eval == False:\n",
    "            results_dict.update({'logits': logits, 'Y_prob': Y_prob, 'Y_hat': Y_hat, 'Instance_loss': instance_loss, 'hover_logits':hover_logits})\n",
    "        elif eval == True:\n",
    "            results_dict.update({'logits': logits, 'Y_prob': Y_prob, 'Y_hat': Y_hat,})\n",
    "\n",
    "\n",
    "        return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Logger(object):\n",
    "    \"\"\"Accuracy logger\"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(Accuracy_Logger, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.data = [{\"count\": 0, \"correct\": 0} for i in range(self.n_classes)]\n",
    "\n",
    "    def log(self, Y_hat, Y):\n",
    "        Y_hat = int(Y_hat)\n",
    "        Y = int(Y)\n",
    "        self.data[Y][\"count\"] += 1\n",
    "        self.data[Y][\"correct\"] += (Y_hat == Y)\n",
    "\n",
    "    def log_batch(self, count, correct, c):\n",
    "        self.data[c][\"count\"] += count\n",
    "        self.data[c][\"correct\"] += correct\n",
    "\n",
    "    def log_batch_rnn(self, Y_hat, Y):\n",
    "        Y_hat = np.array(Y_hat).astype(int)\n",
    "        Y = np.array(Y).astype(int)\n",
    "        for label_class in np.unique(Y):\n",
    "            cls_mask = Y == label_class\n",
    "            self.data[label_class][\"count\"] += cls_mask.sum()\n",
    "            self.data[label_class][\"correct\"] += (Y_hat[cls_mask] == Y[cls_mask]).sum()\n",
    "\n",
    "    def get_summary(self, c):\n",
    "        count = self.data[c][\"count\"]\n",
    "        correct = self.data[c][\"correct\"]\n",
    "\n",
    "        if count == 0:\n",
    "            acc = None\n",
    "        else:\n",
    "            acc = float(correct) / count\n",
    "\n",
    "        return acc, correct, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(Y_hat, Y):\n",
    "    error = 1. - Y_hat.float().eq(Y.float()).float().mean().item()\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=40, stop_epoch=100, verbose=False):  # 连续patience轮，并且总论此超过stop_epoch轮就会终止\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 20\n",
    "            stop_epoch (int): Earliest epoch possible for stopping\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.stop_epoch = stop_epoch\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_max = np.Inf\n",
    "\n",
    "    def __call__(self, epoch, val_loss, model, ckpt_name='checkpoint.pt'):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, ckpt_name)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            logging.info(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience and epoch > self.stop_epoch:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, ckpt_name)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, early_stopping, model, ckpt_name):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            logging.info(\n",
    "                f'Validation loss decreased ({self.val_loss_max:.6f} --> {early_stopping:.6f}).  Saving model ...')\n",
    "        print(\"Save model!!!!!!!!!!!\")\n",
    "        torch.save(model.state_dict(), ckpt_name)\n",
    "        self.val_loss_max = early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_to_excel(fold, loader_name, label_list, probs_list, y_hat_list, accuracy, specificity, sensitivity, precision, f1_score, cls_auc, auprc):\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Patients\": loader_name,\n",
    "            \"labels\": label_list,\n",
    "            \"probs\": probs_list,\n",
    "            \"y_hat\": y_hat_list,\n",
    "            \"Accuracy\" : accuracy,\n",
    "            \"Specificity\": specificity,\n",
    "            \"Sensitivity\":sensitivity,\n",
    "            \"Precision\": precision,\n",
    "            \"F1-score\": f1_score,\n",
    "            \"Auc\": cls_auc,\n",
    "            \"AUPRC\": auprc,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if fold == 0:\n",
    "        with pd.ExcelWriter('./summary/0307/TP53/TP53_0307CV11_clam.xlsx', engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Sheet0', index=False)\n",
    "    else:\n",
    "        with pd.ExcelWriter('./summary/0307/TP53/TP53_0307CV11_clam.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "            df.to_excel(writer, sheet_name='Sheet'+str(fold), index=False)\n",
    "\n",
    "    print(\"Patients data is successfully written into Excel File\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(model, loader, n_classes, fold):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cls_logger = Accuracy_Logger(n_classes=n_classes)\n",
    "    model.eval()\n",
    "    cls_test_error = 0.\n",
    "    cls_test_loss = 0.\n",
    "\n",
    "    all_cls_probs = np.zeros((len(loader), n_classes))\n",
    "    all_cls_labels = np.zeros(len(loader))\n",
    "    all_cls_y_hats = np.zeros(len(loader))\n",
    "\n",
    "    patient_results = {}\n",
    "    \n",
    "    loader_name = []\n",
    "    for batch_idx, (npy_dir, npy_dir_count, label) in enumerate(loader):\n",
    "        data = npy_loader(npy_dir[0]).to(device)\n",
    "        count = npy_loader_count(npy_dir_count[0]).to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():\n",
    "            results_dict = model(data, count, eval=True)\n",
    "\n",
    "        logits, Y_prob, Y_hat = results_dict['logits'], results_dict['Y_prob'], results_dict['Y_hat']\n",
    "        loader_name.append(npy_dir[0])\n",
    "\n",
    "        cls_logger.log(Y_hat, label)\n",
    "        cls_probs = Y_prob.cpu().numpy()\n",
    "        cls_Yhats = Y_hat.cpu().numpy()\n",
    "        all_cls_probs[batch_idx] = cls_probs\n",
    "        all_cls_labels[batch_idx] = label.item()\n",
    "        all_cls_y_hats[batch_idx] = cls_Yhats.item()\n",
    "\n",
    "        cls_error = calculate_error(Y_hat, label)\n",
    "        cls_test_error += cls_error\n",
    "\n",
    "    cls_test_error /= len(loader)\n",
    "\n",
    "    if n_classes == 2:\n",
    "        print(all_cls_labels)\n",
    "        print(all_cls_y_hats)\n",
    "        tn, fp, fn, tp = confusion_matrix(all_cls_labels, all_cls_y_hats, labels=[0, 1]).ravel()\n",
    "        accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "        sensitivity = tp/(tp+fn)\n",
    "        specificity = tn/(tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "        try:\n",
    "            cls_auc = roc_auc_score(all_cls_labels, all_cls_probs[:, 1])\n",
    "            precision1, recall1, _ = precision_recall_curve(all_cls_labels,all_cls_probs[:,1])\n",
    "            auprc = calc_auc(recall1, precision1)\n",
    "        except:\n",
    "            cls_auc = 'nan'\n",
    "            auprc = 'nan'\n",
    "        print(\"Accuracy: \"+str(accuracy))\n",
    "        print(\"Specificity: \"+str(specificity))\n",
    "        print(\"Sensitivity: \"+str(sensitivity))\n",
    "        print(\"Recall: \"+str(recall))\n",
    "        print(\"Precision: \"+str(precision))\n",
    "        print(\"F1-score: \"+str(f1_score))\n",
    "        print(\"Auc: \"+str(cls_auc))\n",
    "        print(\"AUPRC: \"+str(auprc))\n",
    "\n",
    "        summary_to_excel(fold, loader_name, all_cls_labels, all_cls_probs[:,1], all_cls_y_hats, \n",
    "                         accuracy, specificity, sensitivity, precision, f1_score, cls_auc, auprc)\n",
    "    else:\n",
    "        cls_auc = roc_auc_score(all_cls_labels, all_cls_probs[:,1], multi_class='ovr')\n",
    "\n",
    "    return patient_results, cls_test_error, cls_auc, cls_logger, all_cls_labels, all_cls_probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(tprs, mean_fpr):\n",
    "    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = calc_auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "            label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(train_loss, valid_loss, fold):\n",
    "    title = 'MSI HoverAtt fold' + str(fold+1) + ' loss curve'\n",
    "    plt.plot(train_loss, label='train loss')\n",
    "    plt.plot(valid_loss, label='validation loss')\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.savefig('/home/ldap_howard/script/summary/attention_hover/loss_fold'+str(fold)+'_clam.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, model, loader, optimizer, n_classes, writer=None, loss_fn=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    cls_logger = Accuracy_Logger(n_classes=n_classes)\n",
    "    cls_train_error = 0.\n",
    "    cls_train_loss = 0.\n",
    "    train_inst_loss = 0.\n",
    "    total_loss = 0.\n",
    "    for batch_idx, (npy_dir, npy_dir_count, label) in enumerate(loader):\n",
    "        data = npy_loader(npy_dir[0]).to(device)\n",
    "        count = npy_loader_count(npy_dir_count[0]).to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        results_dict = model(data, count)\n",
    "        logits, Y_prob, Y_hat, instance_loss, hover_logits = results_dict['logits'], results_dict['Y_prob'], results_dict['Y_hat'], results_dict['Instance_loss'], results_dict['hover_logits']\n",
    "\n",
    "        cls_logger.log(Y_hat, label)\n",
    "\n",
    "        cls_loss = loss_fn(logits, label)\n",
    "        cls_loss_value = cls_loss.item()\n",
    "\n",
    "        hover_loss = loss_fn(hover_logits, label)\n",
    "        hover_loss_value = hover_loss.item()\n",
    "\n",
    "        total_loss = 0.86*cls_loss + 0.11*hover_loss + 0.03*instance_loss\n",
    "        cls_train_loss += cls_loss.item()\n",
    "\n",
    "        cls_error = calculate_error(Y_hat, label)\n",
    "        cls_train_error += cls_error\n",
    "\n",
    "        # backward pass\n",
    "        total_loss.backward()\n",
    "        # step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # calculate loss and error for epoch\n",
    "    cls_train_loss /= len(loader)\n",
    "    cls_train_error /= len(loader)\n",
    "\n",
    "    logging.info(\n",
    "        'Epoch: {}, cls train_loss: {:.4f}, cls train_error: {:.4f}'.format(epoch, cls_train_loss, cls_train_error))\n",
    "    for i in range(n_classes):\n",
    "        acc, correct, count = cls_logger.get_summary(i)\n",
    "        logging.info('class {}: tpr {:.4f}, correct {}/{}'.format(i, acc, correct, count))\n",
    "        if writer:\n",
    "            writer.add_scalar('train/class_{}_tpr'.format(i), acc, epoch)\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalar('train/cls_loss', cls_train_loss, epoch)\n",
    "        writer.add_scalar('train/cls_error', cls_train_error, epoch)\n",
    "    return cls_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model_name, epoch, model, loader, n_classes, early_stopping=None, writer=None, loss_fn=None, results_dir=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    cls_logger = Accuracy_Logger(n_classes=n_classes)\n",
    "    cls_val_error = 0.\n",
    "    cls_val_loss = 0.\n",
    "\n",
    "    cls_probs = np.zeros((len(loader), n_classes))\n",
    "    cls_labels = np.zeros(len(loader))\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (npy_dir, npy_dir_count, label) in enumerate(loader):\n",
    "            data = npy_loader(npy_dir[0]).to(device)\n",
    "            count = npy_loader_count(npy_dir_count[0]).to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            results_dict = model(data, count, eval=True)\n",
    "            logits, Y_prob, Y_hat = results_dict['logits'], results_dict['Y_prob'], results_dict['Y_hat']\n",
    "            del results_dict\n",
    "\n",
    "            cls_logger.log(Y_hat, label)\n",
    "\n",
    "            cls_loss = loss_fn(logits, label)\n",
    "            cls_loss_value = cls_loss.item()      \n",
    "\n",
    "            cls_probs[batch_idx] = Y_prob.cpu().numpy()\n",
    "            cls_labels[batch_idx] = label.item()\n",
    "\n",
    "            cls_val_loss += cls_loss.item()\n",
    "            cls_error = calculate_error(Y_hat, label)\n",
    "            \n",
    "            cls_val_error += cls_error\n",
    "\n",
    "            all_labels.append(label.detach().cpu().numpy()[0])\n",
    "            all_outputs.append(Y_prob[0][1].detach().cpu().tolist())\n",
    "\n",
    "    cls_val_error /= len(loader)\n",
    "    cls_val_loss /= len(loader)\n",
    "\n",
    "    print(\"cls val loss: \" + str(cls_val_loss))\n",
    "\n",
    "    if n_classes == 2:\n",
    "        cls_auc = roc_auc_score(cls_labels, cls_probs[:, 1])\n",
    "        precision1, recall1, _ = precision_recall_curve(cls_labels,cls_probs[:, 1])\n",
    "        cls_auprc = calc_auc(recall1, precision1)\n",
    "        cls_aucs = []\n",
    "    else:\n",
    "        cls_aucs = []\n",
    "        binary_labels = label_binarize(cls_labels, classes=[i for i in range(n_classes)])\n",
    "        for class_idx in range(n_classes):\n",
    "            if class_idx in cls_labels:\n",
    "                fpr, tpr, _ = roc_curve(binary_labels[:, class_idx], cls_probs[:, class_idx])\n",
    "                cls_aucs.append(calc_auc(fpr, tpr))\n",
    "            else:\n",
    "                cls_aucs.append(float('nan'))\n",
    "\n",
    "        cls_auc = np.nanmean(np.array(cls_aucs))\n",
    "    print(\"cls_auc: \" + str(cls_auc))\n",
    "    print(\"cls_auprc: \" + str(cls_auprc))\n",
    "\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalar('val/cls_loss', cls_val_loss, epoch)\n",
    "        writer.add_scalar('val/cls_auc', cls_auc, epoch)\n",
    "        writer.add_scalar('val/cls_error', cls_val_error, epoch)\n",
    "\n",
    "    logging.info(\n",
    "        '\\nVal Set, cls val_loss: {:.4f}, cls val_error: {:.4f}, cls auc: {:.4f}'.format(cls_val_loss, cls_val_error,\n",
    "                                                                                         cls_auc))\n",
    "    for i in range(n_classes):\n",
    "        acc, correct, count = cls_logger.get_summary(i)\n",
    "        logging.info('class {}: tpr {}, correct {}/{}'.format(i, acc, correct, count))\n",
    "        if writer:\n",
    "            writer.add_scalar('val/class_{}_tpr'.format(i), acc, epoch)\n",
    "    print(model_name)\n",
    "\n",
    "    if early_stopping:\n",
    "        assert results_dir\n",
    "        early_stopping(epoch, cls_val_loss, model,\n",
    "                       ckpt_name=os.path.join(results_dir, model_name))\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            logging.info(\"Early stopping\")\n",
    "            return cls_val_loss, True\n",
    "\n",
    "    return cls_val_loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SimpleDataset(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "early_stopping = None\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "tprs = []\n",
    "aucs = []\n",
    "auprcs = []\n",
    "precisions = []\n",
    "y_real = []\n",
    "y_probs = []\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "mean_recall = np.linspace(0,1,100)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,20))\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kf.split(train_x, train_y)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    model_name = './model/TP53_0307CV11_fold'+str(fold)+'_clam_checkpoint.pt'\n",
    "    train_fold = torch.utils.data.Subset(train_data, train_ids)\n",
    "    valid_fold = torch.utils.data.Subset(train_data, valid_ids)\n",
    "\n",
    "    train_loader = DataLoader(train_fold, batch_size=1, shuffle = True)\n",
    "    valid_loader = DataLoader(valid_fold, batch_size=1)\n",
    "    \n",
    "    model = Attention()\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = FocalLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00006, betas=(0.9, 0.999), weight_decay=0.003)\n",
    "\n",
    "    writer = None\n",
    "    early_stopping = EarlyStopping(patience=30, stop_epoch=100, verbose=True)\n",
    "\n",
    "    results_dir = '/home/ldap_howard/script/'\n",
    "    cur = 'model'\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "\n",
    "    for epoch in range(2000):\n",
    "        print(epoch)\n",
    "        train_epoch_loss = train_loop(epoch, model, train_loader, optimizer, 2, writer, loss_fn)\n",
    "        valid_epoch_loss, stop = validate(model_name, epoch, model, valid_loader, 2,\n",
    "                                early_stopping, writer, loss_fn, results_dir)\n",
    "        \n",
    "        train_loss.append(train_epoch_loss)\n",
    "        valid_loss.append(valid_epoch_loss)\n",
    "\n",
    "        if stop:\n",
    "            break\n",
    "    \n",
    "    if early_stopping:\n",
    "        print(\"EarlyStopping\")\n",
    "        model.load_state_dict(torch.load(os.path.join(results_dir, model_name))) #\"s_{}_checkpoint.pt\".format(cur))))\n",
    "    else:\n",
    "        torch.save(model.state_dict(), os.path.join(results_dir, model_name)) #\"s_{}_{}_checkpoint.pt\".format(cur)))\n",
    "\n",
    "    _, cls_val_error, cls_val_auc, _, cls_labels, cls_probs = summary(model, valid_loader, 2, fold)\n",
    "    fpr, tpr, t = roc_curve(cls_labels, cls_probs)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = calc_auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "    lab_fold = 'Fold %d AUROC=%.3f' % (i+1, roc_auc)\n",
    "    ax1.plot(fpr, tpr, alpha=0.8, lw=4, label=lab_fold)\n",
    "\n",
    "    precision_fold, recall_fold, _ = precision_recall_curve(cls_labels, cls_probs)\n",
    "    precision_fold, recall_fold = precision_fold[::-1], recall_fold[::-1]\n",
    "    precisions.append(np.interp(mean_recall, recall_fold, precision_fold))\n",
    "    auprc = calc_auc(recall_fold, precision_fold)\n",
    "    auprcs.append(auprc)\n",
    "    y_real.append(cls_labels)\n",
    "    y_probs.append(cls_probs)\n",
    "    \n",
    "    lab_fold = 'Fold %d AUPRC=%.3f' % (i+1, auprc)\n",
    "    ax2.plot(recall_fold, precision_fold, alpha=0.8, lw=4, label=lab_fold)\n",
    "\n",
    "    i=i+1\n",
    "    logging.info('Cls Val error: {:.4f}, Cls ROC AUC: {:.4f}'.format(cls_val_error, cls_val_auc))\n",
    "    print(\"Validation acc: \" + str(cls_val_auc))\n",
    "    print(\"validation error: \" + str(cls_val_error))\n",
    "\n",
    "    #plot_loss_curve(train_loss, valid_loss, fold)\n",
    "\n",
    "ax1.set_title('AUROC',fontsize=20)\n",
    "ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "ax1.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=f\"ROC curves\",\n",
    ")\n",
    "ax1.set_xticks(np.arange(0, 1.1, 0.2))\n",
    "ax1.set_yticks(np.arange(0, 1.1, 0.2))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax1.set_xlabel(\"False Positive Rate\", fontsize=24)\n",
    "ax1.set_ylabel(\"True Positive Rate\", fontsize=24)\n",
    "ax1.set_title(\"HoverAtt(TP53)\", fontsize=32)\n",
    "ax1.legend(loc=\"lower right\", fontsize = 20)\n",
    "\n",
    "ax2.set_title('AUPRC',fontsize=20)\n",
    "ax2.plot([0, 1], [1, 0], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "ax2.set(\n",
    "    xlabel=\"Recall\",\n",
    "    ylabel=\"Precision\",\n",
    "    title=f\"PR curves\",\n",
    ")\n",
    "ax2.set_xticks(np.arange(0, 1.1, 0.2))\n",
    "ax2.set_yticks(np.arange(0, 1.1, 0.2))\n",
    "ax2.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax2.set_xlabel(\"Recall\", fontsize=24)\n",
    "ax2.set_ylabel(\"Precision\", fontsize=24)\n",
    "ax2.set_title(\"HoverAtt(TP53)\", fontsize=32)\n",
    "ax2.legend(loc='lower left', fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('/home/ldap_howard/script/summary/attention_hover/TP53_clam.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSIL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_excel('/home/ldap_howard/script/MSI_CRC_DX_0307_NA.xlsx', sheet_name='MSIL')\n",
    "test_x = test_dataset['Patients'].values\n",
    "test_y = test_dataset['isMSIH'].values\n",
    "test_data = SimpleDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (layer1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (attention_V): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (attention_U): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (attention_weights): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       "  (instance_loss): FocalLoss()\n",
       "  (fc_c): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=2, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (fc_X): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=2, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Attention().to(device)\n",
    "model.load_state_dict(torch.load('/home/ldap_howard/script/model/MSI_0307CV13_fold1_clam_checkpoint.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "Accuracy: 0.8493975903614458\n",
      "Specificity: 0.9027777777777778\n",
      "Sensitivity: 0.5\n",
      "Recall: 0.5\n",
      "Precision: 0.44\n",
      "F1-score: 0.46808510638297873\n",
      "Auc: 0.7099116161616161\n",
      "AUPRC: 0.398958317766341\n",
      "Patients data is successfully written into Excel File\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({},\n",
       " 0.15060240963855423,\n",
       " 0.7099116161616161,\n",
       " <__main__.Accuracy_Logger at 0x7fa37611afb0>,\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.64317197, 0.66073507, 0.27932507, 0.31739086, 0.55644733,\n",
       "        0.60599965, 0.29191434, 0.27950007, 0.32653153, 0.59652179,\n",
       "        0.28533968, 0.28427517, 0.50997746, 0.4666315 , 0.57573724,\n",
       "        0.65468282, 0.6451723 , 0.47588056, 0.32448846, 0.50731426,\n",
       "        0.63864958, 0.27609703, 0.27725384, 0.29204428, 0.30951491,\n",
       "        0.63583469, 0.28955665, 0.41884622, 0.29580241, 0.32198891,\n",
       "        0.28238249, 0.28132769, 0.56760997, 0.38069913, 0.28010648,\n",
       "        0.40472886, 0.32945454, 0.30391273, 0.33175766, 0.48225096,\n",
       "        0.38445577, 0.35328001, 0.34262499, 0.30428454, 0.2908909 ,\n",
       "        0.44333145, 0.45080587, 0.28105551, 0.28338102, 0.37024632,\n",
       "        0.63517886, 0.64225566, 0.27158123, 0.2797578 , 0.27387086,\n",
       "        0.29303446, 0.28377327, 0.33877468, 0.60084903, 0.59416491,\n",
       "        0.38679323, 0.31684288, 0.33640766, 0.27555355, 0.2716167 ,\n",
       "        0.28338709, 0.27751124, 0.27633494, 0.448578  , 0.38745755,\n",
       "        0.27837005, 0.29395753, 0.29264131, 0.27623999, 0.30734837,\n",
       "        0.27989388, 0.27560291, 0.33786261, 0.27974689, 0.2882466 ,\n",
       "        0.28708529, 0.43506283, 0.52275306, 0.27506888, 0.2989085 ,\n",
       "        0.27943587, 0.283198  , 0.28111896, 0.27988234, 0.3422628 ,\n",
       "        0.28469861, 0.27771726, 0.28516456, 0.27824637, 0.27644473,\n",
       "        0.40831876, 0.56682432, 0.28109214, 0.30771717, 0.30803397,\n",
       "        0.32954574, 0.60023749, 0.32366586, 0.46668929, 0.28358188,\n",
       "        0.29149902, 0.30047214, 0.32652342, 0.2787478 , 0.40062398,\n",
       "        0.35904628, 0.27597007, 0.28535572, 0.51417428, 0.28174898,\n",
       "        0.31462759, 0.31148714, 0.27980167, 0.29437116, 0.65595996,\n",
       "        0.30187511, 0.32201308, 0.27797714, 0.28655383, 0.29416203,\n",
       "        0.27792621, 0.27703479, 0.3450352 , 0.2885052 , 0.29018298,\n",
       "        0.28125799, 0.2756606 , 0.31900296, 0.38392955, 0.27689007,\n",
       "        0.31794858, 0.29665542, 0.32747668, 0.3655439 , 0.34908399,\n",
       "        0.30754924, 0.32582372, 0.61405647, 0.32940057, 0.29205474,\n",
       "        0.2862837 , 0.28102452, 0.49763253, 0.35732338, 0.28972277,\n",
       "        0.29333973, 0.35349619, 0.31809995, 0.31356147, 0.32068568,\n",
       "        0.28813031, 0.6357798 , 0.35489103, 0.28597113, 0.54688096,\n",
       "        0.27284801, 0.34929156, 0.27384812, 0.31212586, 0.30045956,\n",
       "        0.28109336]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, test_loader, 2, 'CRC_NA_MSIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.path = '/ORCA_lake/TCGA-COAD/feature/PAIP_0307/'\n",
    "        self.path_c = '/ORCA_lake/TCGA-COAD/hovernet_kmeans/PAIP_0307_MI2N/'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)   \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.path, self.x[idx]+'.npy')\n",
    "        count_path = os.path.join(self.path_c, self.x[idx]+'.npy')\n",
    "\n",
    "        return image_path, count_path, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_excel('/home/ldap_howard/script/MSI_PAIP.xlsx')\n",
    "test_x = test_dataset['Patients'].values\n",
    "test_y = test_dataset['isMSIH'].values\n",
    "test_data = SimpleDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (layer1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (attention_V): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (attention_U): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (attention_weights): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       "  (instance_loss): FocalLoss()\n",
       "  (fc_c): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=2, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (fc_X): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=2, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Attention().to(device)\n",
    "model.load_state_dict(torch.load('/home/ldap_howard/script/model/MSI_0307CV26_fold3_clam_checkpoint.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Accuracy: 0.8085106382978723\n",
      "Specificity: 1.0\n",
      "Sensitivity: 0.25\n",
      "Recall: 0.25\n",
      "Precision: 1.0\n",
      "F1-score: 0.4\n",
      "Auc: 0.9523809523809523\n",
      "AUPRC: 0.9081551732867522\n",
      "Patients data is successfully written into Excel File\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({},\n",
       " 0.19148936170212766,\n",
       " 0.9523809523809523,\n",
       " <__main__.Accuracy_Logger at 0x7fb28c28d8d0>,\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.57398266, 0.45270523, 0.38745889, 0.65565842, 0.32585603,\n",
       "        0.2735382 , 0.48353097, 0.35349894, 0.27467501, 0.32804441,\n",
       "        0.45700932, 0.65908653, 0.27158386, 0.26972786, 0.27510515,\n",
       "        0.2721498 , 0.27069366, 0.27129275, 0.27123681, 0.33996898,\n",
       "        0.27982265, 0.2879568 , 0.27321532, 0.27591282, 0.27932218,\n",
       "        0.27228701, 0.27022722, 0.27123752, 0.27205902, 0.27101848,\n",
       "        0.27168006, 0.27867323, 0.27108306, 0.27102104, 0.26938   ,\n",
       "        0.27347121, 0.27040312, 0.27233604, 0.27585796, 0.27051467,\n",
       "        0.27100447, 0.27219978, 0.26967797, 0.26970273, 0.27327457,\n",
       "        0.27806199, 0.2701208 ]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, test_loader, 2, 'PAIP3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPTAC ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.path = '/ORCA_lake/TCGA-COAD/feature/CPTAC_0307/'\n",
    "        self.path_c = '/ORCA_lake/TCGA-COAD/hovernet_kmeans/CPTAC_0307_MI2N/'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)   \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.path, self.x[idx]+'.npy')\n",
    "        count_path = os.path.join(self.path_c, self.x[idx]+'.npy')\n",
    "\n",
    "        return image_path, count_path, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_excel('/home/ldap_howard/script/MSI_CPTAC_0307.xlsx')\n",
    "test_x = test_dataset['Patients'].values\n",
    "test_y = test_dataset['isMSIH'].values\n",
    "test_data = SimpleDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (layer1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (attention_V): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (attention_U): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (attention_weights): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       "  (instance_loss): FocalLoss()\n",
       "  (fc_X): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (fc_c): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=2, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Attention().to(device)\n",
    "model.load_state_dict(torch.load('/home/ldap_howard/script/model/MSI_0307CV5_fold1_clam_checkpoint.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "Accuracy: 0.8090909090909091\n",
      "Specificity: 0.875\n",
      "Sensitivity: 0.5961538461538461\n",
      "Recall: 0.5961538461538461\n",
      "Precision: 0.5961538461538461\n",
      "F1-score: 0.5961538461538461\n",
      "Auc: 0.8222298534798536\n",
      "AUPRC: 0.6434037679611059\n",
      "Patients data is successfully written into Excel File\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({},\n",
       " 0.19090909090909092,\n",
       " 0.8222298534798536,\n",
       " <__main__.Accuracy_Logger at 0x7fb28c28d2d0>,\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.65881479, 0.68456101, 0.67023468, 0.47418323, 0.60802424,\n",
       "        0.27258882, 0.5681743 , 0.4251976 , 0.27283049, 0.67386484,\n",
       "        0.66168737, 0.66490662, 0.66957104, 0.32194468, 0.555269  ,\n",
       "        0.48885629, 0.60869682, 0.65510112, 0.32160634, 0.29082224,\n",
       "        0.44203997, 0.67374396, 0.36066586, 0.67390209, 0.46378461,\n",
       "        0.58089167, 0.31562504, 0.56614441, 0.30362582, 0.55117035,\n",
       "        0.44033912, 0.28004742, 0.55358964, 0.37275118, 0.31841969,\n",
       "        0.62882578, 0.43291104, 0.45575413, 0.46693534, 0.61808604,\n",
       "        0.6690163 , 0.66099465, 0.49595496, 0.64929008, 0.57102424,\n",
       "        0.51649463, 0.62994504, 0.5968262 , 0.54046297, 0.66287893,\n",
       "        0.5164361 , 0.52514422, 0.33690369, 0.43828312, 0.32719937,\n",
       "        0.2961618 , 0.49197587, 0.29938963, 0.28055623, 0.31265512,\n",
       "        0.41498262, 0.34590572, 0.46803439, 0.2725746 , 0.2835539 ,\n",
       "        0.27321076, 0.41001931, 0.3423734 , 0.28629908, 0.27355191,\n",
       "        0.32881495, 0.30009741, 0.40480277, 0.30579975, 0.30494258,\n",
       "        0.29087967, 0.27766109, 0.27822948, 0.29829544, 0.61705041,\n",
       "        0.36291224, 0.63513201, 0.28267628, 0.60025519, 0.5652343 ,\n",
       "        0.28279078, 0.2773543 , 0.27001721, 0.27309278, 0.37913632,\n",
       "        0.30367097, 0.28026685, 0.28200743, 0.27740479, 0.28103861,\n",
       "        0.27808836, 0.27826008, 0.31319648, 0.2953243 , 0.65767503,\n",
       "        0.35060614, 0.66580874, 0.5882315 , 0.63065511, 0.28319716,\n",
       "        0.32393232, 0.5469445 , 0.6284011 , 0.29766366, 0.36983597,\n",
       "        0.29032373, 0.41399133, 0.29494593, 0.48944804, 0.65313309,\n",
       "        0.63021958, 0.30481613, 0.48816562, 0.36145115, 0.27448204,\n",
       "        0.34619561, 0.53114539, 0.42650488, 0.35558227, 0.28909078,\n",
       "        0.31512049, 0.56976694, 0.55662674, 0.40173209, 0.30386293,\n",
       "        0.31851542, 0.58863688, 0.54089993, 0.56477791, 0.42525685,\n",
       "        0.31435761, 0.28332362, 0.29575911, 0.30368775, 0.46101597,\n",
       "        0.26919428, 0.30501071, 0.30119082, 0.28150216, 0.28104848,\n",
       "        0.32333791, 0.45259941, 0.42816722, 0.40740511, 0.2851733 ,\n",
       "        0.28216085, 0.37780416, 0.39860007, 0.28274772, 0.27305266,\n",
       "        0.26932055, 0.27896389, 0.27678409, 0.27350128, 0.27074099,\n",
       "        0.27136901, 0.308792  , 0.34189677, 0.27967975, 0.26980612,\n",
       "        0.31502458, 0.39516357, 0.38357824, 0.35073841, 0.28796813,\n",
       "        0.29143786, 0.28286663, 0.45787603, 0.27415156, 0.28240985,\n",
       "        0.30235231, 0.27811116, 0.61592412, 0.27707037, 0.27777514,\n",
       "        0.28106821, 0.27512482, 0.26916239, 0.28483838, 0.28618288,\n",
       "        0.27943587, 0.28725573, 0.27915862, 0.31942216, 0.44936147,\n",
       "        0.36396351, 0.29199812, 0.29259372, 0.29115337, 0.303496  ,\n",
       "        0.29324228, 0.43598428, 0.27250347, 0.56972158, 0.30751473,\n",
       "        0.31501192, 0.28060049, 0.40995607, 0.2909475 , 0.57121015,\n",
       "        0.39821032, 0.38927481, 0.29360771, 0.29302943, 0.40689057,\n",
       "        0.45834127, 0.31006762, 0.27663809, 0.32903188, 0.29434273,\n",
       "        0.31558141, 0.32593775, 0.2761634 , 0.27605724, 0.32117093]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, test_loader, 2, 'CPTAC3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
